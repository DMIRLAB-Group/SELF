{
    "collab_server" : "",
    "contents" : "score_ml_cd<-function(G,D,...){\n  NodeScore<-rep(0,ncol(G))\n  NodeScore<-updateScore_ml_cd(G,D,NodeScore,1:ncol(G),...)\n  return(NodeScore)\n}\nupdateScore_ml_cd<-function(G,D,NodeScore,nodes,score_type=\"bic\",bw =\"nrd0\",booster=\"gbtree\",gamma=0,nrounds=30,...){\n  bicterm=log(Dn)/Dn/2\n  for(i in nodes){\n    pa<-parents(G,i)\n    if(length(pa)==0){\n      N<-D[,i]\n      if(is.factor(D[,i])){\n        si=length(unique(D[,i]))\n        d=si-1\n        eta=unlist(table(N)/length(N))\n        if(score_type==\"log\"){\n          NodeScore[i]<-sum(eta*log(eta)) #log\n        }else if(score_type==\"bic\"){\n          NodeScore[i]<-sum(eta*log(eta))-d*bicterm #bic\n        }else if(score_type==\"aic\"){\n          NodeScore[i]<-sum(eta*log(eta))-d/Dn #aic\n        }\n        next\n      }else{\n        if(score_type==\"log\"){\n          f <- approxfun(density(N,bw=bw,from=min(N),to=max(N)))\n          NodeScore[i]<-sum(log(f(N)))/Dn\n        }else if(score_type==\"bic\"){\n          f <- approxfun(density(N,bw=bw,from=min(N),to=max(N)))\n          NodeScore[i]<-sum(log(f(N)))/Dn\n        }else if(score_type==\"aic\"){\n          f <- approxfun(density(N,bw=bw,from=min(N),to=max(N)))\n          NodeScore[i]<-sum(log(f(N)))/Dn-1\n        }\n\n        next\n      }\n    }\n    x<-D[,pa,drop=F]\n    y<-D[,i]\n    x=predict(dummyVars(~.,data=x,fullRank = T),x)\n\n\n    if(is.factor(y)){\n      collevels<-na.omit(levels(y))\n      y<-factor(y,labels=0:(length(levels(y))-1))\n    }\n    x<-apply(x,2,as.numeric)\n    x<-as.data.frame(x)\n    if(is.factor(y)){\n        num_class<-length(unique(y))\n        model<-xgboost(data=as.matrix(x),label=as.integer(y)-1,\n                       objective=\"multi:softmax\",num_class=num_class,verbose=0,\n                       nrounds=nrounds,gamma=gamma,save_period=NULL,...)\n    }else{\n\n      model<-xgboost(data=as.matrix(x),label=as.numeric(y),\n                     verbose=0,nrounds=nrounds,gamma=gamma,save_period=NULL,...)\n    }\n    y_hat<-predict(model,as.matrix(x))\n    #xgb---end\n    if(is.factor(y)){\n        N<-as.numeric(y)-as.numeric(y_hat)-1\n    }else{\n      N<-as.numeric(y)-as.numeric(y_hat)\n    }\n\n\n\n    if(is.factor(y)){\n      pa<-parents(G,i)\n      pa_name<-names(D)[pa]\n      dt<-as.data.table(D)\n      ri<-nrow(dt[,.N,by=pa_name])\n      #ri<-nrow(ddply(D,as.quoted(pa_name),length))\n      si=length(unique(D[,i]))\n      if(length(ri)==0){\n        ri=1\n      }\n      d=ri*(si-1)\n      eta=unlist(table(N)/length(N))\n      if(score_type==\"log\"){\n        NodeScore[i]<-sum(eta*log(eta)) #log\n      }else if(score_type==\"bic\"){\n        NodeScore[i]<-sum(eta*log(eta))-d*bicterm #bic\n      }else if(score_type==\"aic\"){\n        NodeScore[i]<-sum(eta*log(eta))-d/Dn #aic\n      }\n\n    }else{\n      if(booster==\"gbtree\"){\n        dump=xgb.dump(model)\n        d=length(grep(dump,pattern = \"leaf\"))\n        d=d+length(pa) #add new feature for penalty term\n      }else{\n        d=length(pa)+1  #gblinear\n      }\n      if(score_type==\"log\"){\n        f <- approxfun(density(N,bw=bw,from=min(N),to=max(N)))\n        NodeScore[i]<-sum(log(f(N)))/Dn\n      }else if(score_type==\"bic\"){\n        f <- approxfun(density(N,bw=bw,from=min(N),to=max(N)))\n        NodeScore[i]<-sum(log(f(N)))/Dn-d*bicterm\n      }else if(score_type==\"aic\"){\n        f <- approxfun(density(N,bw=bw,from=min(N),to=max(N)))\n        NodeScore[i]<-sum(log(f(N)))/Dn-bicterm\n      }\n\n    }\n  }\n\n  return(NodeScore)\n}\n#' @title fast hill-climbing\n#' @description The function for the causal structure learning.\n#' @param D Input Data,your data require to be numeric or factor.\n#' @param G An initial graph for hill climb\n#' @param min_increase Minimum score increase\n#' @param min_history_diff Minimum history difference\n#' @param h The amount of lag in history difference\n#' @param score_type Your can choise \"bic\",\"log\",\"aic\" score. Default: bic\n#' @param file Specifies the output folder and its path.\n#' @param verbose Show the progress bar for each iteration.\n#' @param isoutput Is output the meta data during the iteration so thar your can easyly restore the last progress and evaluate the model during iteration.\n#' @param bw the smoothing bandwidth which is the parameter of the function density(Kernel Density Estimation)\n#' @param booster which booster to use, can be gbtree or gblinear. Default: gbtree\n#' @param gamma minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be.\n#' @param nrounds the max number of iterations for xgboost.\n#' @param ... other parameters for xgboost.see also: help(xgboost)\n#' @return The adjacency matrix of the casual structure.\n#' @export\n#' @examples\n#' #x->y->z\n#' set.seed(0)\n#' x=rnorm(4000)\n#' y=x^2+runif(4000,-1,1)*0.1\n#' z=y^2+runif(4000,-1,1)*0.1\n#' data=data.frame(x,y,z)\n#' fhc(data,gamma=10,booster = \"gbtree\",isoutput=F)\n#'\n#' #x->y->z linear\n#' set.seed(0)\n#' x=rnorm(4000)\n#' y=3*x+runif(4000,-1,1)*0.1\n#' z=3*y+runif(4000,-1,1)*0.1\n#' data=data.frame(x,y,z)\n#' fhc(data,min_increase=0.01,booster = \"gblinear\",isoutput=F)\n#'\n#' #x<-y->z\n#' set.seed(0)\n#' y=rnorm(4000)\n#' x=y^2+runif(4000,-1,1)*0.1\n#' z=y^2+runif(4000,-1,1)*0.1\n#' data=data.frame(x,y,z)\n#' fhc(data,gamma=10,booster = \"gbtree\",isoutput=F)\n\nfhc<-function(D,G=NULL,min_increase=0,min_history_diff=0,h=5,score_type=\"bic\",\n                      file=\"\",verbose=TRUE,isoutput=FALSE,\n                      bw =\"nrd0\",booster=\"gbtree\",gamma=0,nrounds=30,...){\n  if(h<2)error(\"h should be greater than 2\")\n  if(is.null(G)){\n    G=matrix(0,nrow=ncol(D),ncol=ncol(D))\n    initG=G\n  }else{\n    initG<-G\n  }\n  if(ncol(G)!=ncol(D)){\n    stop(\"the number of nodes should be consistent to the data\")\n  }\n  if(file!=\"\"){\n    if(!file.exists(file)){\n      dir.create(file)\n    }\n    path=paste0(file,\"/\")\n  }else{\n    path=\"\"\n  }\n  Vn<<-ncol(G)\n  Dn<<-nrow(D)\n  t=1\n\n  file<-sprintf(paste0(path,\"GD%s.RData\"),t)\n  while (file.exists(file)) {\n    t=t+1\n    file<-sprintf(paste0(path,\"GD%s.RData\"),t)\n  }\n  if(t==1){\n    if(file.exists(paste0(path,\"initResult\"))){\n      load(paste0(path,\"initResult\"))\n    }else{\n      initResult<-list()\n      history_score=rep(0,h)\n      initResult$G<-G\n      initResult$D<-D\n      initResult$NodeScore<-score_ml_cd(initResult$G,initResult$D,bw =\"nrd0\",nrounds=30,\n                                        gamma=0,score_type=\"bic\",booster=\"gbtree\",...)\n      initResult$score<-sum(initResult$NodeScore)\n      if(isoutput){\n        save(initResult,file = paste0(path,\"initResult.RData\"))\n      }\n\n    }\n    t=0\n  }else{\n    t=t-1\n    file<-sprintf(paste0(path,\"GD%s.RData\"),t)\n    load(file)\n    if(h>length(history_score)){\n      history_score<-c(history_score,rep(0,h-length(history_score)))\n    }\n  }\n  if(!is.matrix(G)){\n    G<-as.matrix(G)\n  }\n  #bestScore=initResult$score\n  bestScore=-Inf\n  result=initResult\n  bestResult=initResult\n\n  repeat{\n    oldG<-G\n    bestG<-G\n    GList<-list()\n    for (i in 1:ncol(G)) {\n      for (j in 1:ncol(G)) {\n        if(i!=j){\n          GList<-append(GList,AddDelReverseLine(G,i,j))\n        }\n      }\n    }\n    GList<-unique(GList)\n    for(i in 1:length(GList)){\n      if(all(GList[[i]]==initG)){\n        GList[[i]]<-NULL\n        break\n      }\n    }\n    if(verbose){\n      pb <- txtProgressBar(0,length(GList),style = 3)\n    }\n    bestIndex=0\n    for(k in seq.int(length.out = length(GList))){\n      if(verbose){\n        #print(sprintf(\"k=%d/%d\",k,length(GList)))\n        setTxtProgressBar(pb, k)\n      }\n      nodes<-compareG(G,GList[[k]])\n      result$G<-GList[[k]]\n      result$NodeScore<-updateScore_ml_cd(GList[[k]],D,initResult$NodeScore,nodes,\n                                          bw =\"nrd0\",nrounds=30,gamma=0,score_type=\"bic\",\n                                          booster=\"gbtree\",...)\n      result$score<-sum(result$NodeScore)\n      score<-result$score\n      if(score>bestScore){\n        bestScore=score\n        bestResult=result\n        bestIndex=k\n        oldG<-bestG\n        bestG<-GList[[k]]\n      }\n    }\n    G<-bestG\n    len=length(history_score)\n    history_score[2:len]<-history_score[1:(len-1)]\n    history_score[1]=bestResult$score\n    if(abs(initResult$score-bestResult$score)<min_increase)break\n    if(diff(range(diff(history_score)))<min_history_diff)break\n    initResult<-bestResult\n    if(verbose){\n      print(paste0(c(\"bestscore:\",sum(bestScore))))\n    }\n\n    t=t+1\n    output<-sprintf(paste0(path,\"GD%s.RData\"),t)\n    if(isoutput){\n      save(G,D,initResult,t,history_score,file=output)\n    }\n\n    if(all(oldG==G))break\n\n  }\n  return(bestG)\n}\n",
    "created" : 1493564872530.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4144510233",
    "id" : "64D42FB1",
    "lastKnownWriteTime" : 1493625918,
    "last_content_update" : 1493625918731,
    "path" : "~/bicanm/R/fast_hill_climb.R",
    "project_path" : "R/fast_hill_climb.R",
    "properties" : {
        "docOutlineVisible" : "1",
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}